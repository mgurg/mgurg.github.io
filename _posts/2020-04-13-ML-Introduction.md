---
layout: post
title: "ML: Wprowadzenie"
categories: ML
author: "MichaÅ‚"
---

https://renanmf.com/machine-learning-and-deep-learning-software-engineers/


## Machine learning - podziaÅ‚

Supervised Learning:

- Classification
- Regression

Unsupervised Learning:

- Clustering
- Dimensionality reduction

Reinforcement learning





# Regresja liniowa

y = ax+b

- a - pochylenie
- b - odsuniÄ™cie na osi y

Rodzaje:

- Korelacja
  - Represents the relationship between two variables
  - Shows that two variables move together (no matter in which direction)
  - A single point (a number)
- Regresja
  - Represents the relationship between two or more variables
  - Shows cause and effect (one variable is affected by the other)
  - One way â€“ â€“ there is always only one variable that is causally dependent
  - A line (in 2D space)



RozwiÄ…zanie nieliniowych problemÃ³w:

- non linear regression
- exponential transformation
- Log transformation (semi-log [x lub y]: gdy X wzrasta o 1 to Y wzrasta o b %, log-log [x i y]:gdy X wzrasta o 1% to Y wzrasta o b % )



Modele

- liniowe
- nieliniowe
  - quadratic
  - expotential
  - logistic

Supervised Learning

zawiera input (feature) [x] i output (target) [y]



### Underfitting - Overfiting

*Overfiting* - przeuczenie na danych treningowych. Zbyt maÅ‚e uogÃ³lnienie modelu, za duÅ¼e skoncentrowanie siÄ™ na danych treningowych. High training accuracy. Lapie rÃ³wnieÅ¼ wszystkie szumy

*Underfiting* - zbyt ogÃ³lny model, nie Å‚apie o co chodzi nawet na treningowym. maÅ‚o dokÅ‚adny. Low train accuracy.  



## Clustering

- Flat (K-Means)
- Hierarchical
  - Agglomerative (Bottom-up)
  - Divisive (Top-Down)

ðŸ“º [How does k-nearest neighbors work?](https://www.youtube.com/watch?v=0p0o5cmgLdE)



# Sieci neuronowe (ANN)

Architektura najprostszej sieci neuronowej (MLP):

![https://www.ritchievink.com/img/post-9-mlp/nn_diagram_1.png](https://www.ritchievink.com/img/post-9-mlp/nn_diagram_1.png)

Å¹rÃ³dÅ‚o: [Programming a neural network from scratch](https://www.ritchievink.com/blog/2017/07/10/programming-a-neural-network-from-scratch/)

**Input layer** - DostarczajÄ… do sieci informacje ze Å›wiata zewnÄ™trznego W Å¼adnym z wÄ™zÅ‚Ã³w wejÅ›ciowych nie sÄ… wykonywane Å¼adne obliczenia - po prostu przekazujÄ… one informacje do ukrytych wÄ™zÅ‚Ã³w.

**Hidden layer** - Nie majÄ… bezpoÅ›redniego poÅ‚Ä…czenia ze Å›wiatem zewnÄ™trznym (stÄ…d nazwa "ukryte"). WykonujÄ… one obliczenia i przekazujÄ… informacje z wÄ™zÅ‚Ã³w wejÅ›ciowych do wÄ™zÅ‚Ã³w wyjÅ›ciowych. Kolekcja ukrytych wÄ™zÅ‚Ã³w tworzy "UkrytÄ… warstwÄ™". SieÄ‡ neuronowa ma tylko jednÄ… warstwÄ™ wejÅ›ciowÄ… i jednÄ… warstwÄ™ wyjÅ›ciowÄ…, moÅ¼e ona mieÄ‡ zerowÄ… (*Single Layer Perceptron*) lub wielowarstwowÄ… (*Multi Layer Perceptron*) czÄ™Å›Ä‡ ukrytÄ….

**Output layer** - SÄ… odpowiedzialne za obliczenia i przesyÅ‚anie informacji z sieci do Å›wiata zewnÄ™trznego.

NajczÄ™stsze typy aktywacji

- *Sigmoid* (funkcja logistyczna). Zakres : (0,1) 
- *TanH* (tangens hiperboliczny). Zakres (-1, 1)
- *ReLu* (rectified linear unit). Zakres (0, inf)
- *SoftMax*. Zakres:  (0,1) Suma jest rÃ³wna 1 (prawdopodobieÅ„stwo)

PrzejÅ›cie od warstwy wejÅ›ciowej do wyjÅ›ciowej to *Feed Forward*, Droga powrotna (OUT -> IN)  to *Back Propagation*. SieÄ‡ uczy siÄ™ i optymalizuje swÃ³j stan wewnÄ™trzny poprzez minimalizowanie funkcji straty.  Backdrop jest potrzebny Å¼eby ustaliÄ‡ wÅ‚aÅ›ciwe wagi i bias. Wykorzystuje gradienty, a Å¼eby uniknÄ…Ä‡ pozostania w lokalnym minimum stosuje siÄ™ [Momentum](https://distill.pub/2017/momentum/).

**Epoch** is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE

**Batch** - is a number of samples fed to NN during single iteration. Total number of training examples present in a single batch. 

**Iterations** is the number of batches needed to complete one epoch.

[Epoch = Batch * Iterations](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)

### Normalizacja

WiÄ™cej: [Batch Normalization â€” Speed up Neural Network Training](https://medium.com/@ilango100/batch-normalization-speed-up-neural-network-training-245e39a62f85)

- Batch Gradient Descent  (Batch Size == Number of samples) 

- Mini-Batch Gradient Descent  ( 1 < Batch Size < Number of samples)

- Stochasting Gradient Descent ( Batch Size ==1)



## CNN

Warstwa konwolucyjna + pooling + ANN = CNN

```python
# simple_cnn
Sequential([
	Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),
	MaxPool2D(pool_size=(2, 2)),
	Dropout(0.25), 
       
	Flatten(), # spÅ‚aszczanie, aby poÅ‚Ä…czyÄ‡ warstwy konwolucyjne z fully connected layers
        
	Dense(1024, activation='relu'),
	Dropout(0.5),
	Dense(num_classes, activation='softmax')
    ])
```



Konwolucja - nakÅ‚adanie filtru

Pooling - sposÃ³b na  zmniejszanie zdjÄ™Ä‡ przy jednoczesnym zachowaniu najwaÅ¼niejszych informacji w nich zawartych.



